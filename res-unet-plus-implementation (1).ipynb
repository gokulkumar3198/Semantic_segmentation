{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport cv2\nimport albumentations as albu\n\nimport time\nimport os\nfrom tqdm import tqdm\n\n# from torchsummary import summary\n# import segmentation_models_pytorch as smp\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:51.370027Z","iopub.execute_input":"2022-06-06T08:55:51.370884Z","iopub.status.idle":"2022-06-06T08:55:55.179464Z","shell.execute_reply.started":"2022-06-06T08:55:51.370749Z","shell.execute_reply":"2022-06-06T08:55:55.178739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.181168Z","iopub.execute_input":"2022-06-06T08:55:55.181426Z","iopub.status.idle":"2022-06-06T08:55:55.246798Z","shell.execute_reply.started":"2022-06-06T08:55:55.181392Z","shell.execute_reply":"2022-06-06T08:55:55.245422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_IMAGE_PATH = '../input/dataset-new/Data_Benign_plus_ET/Train/'\nTrain_MASK_PATH = '../input/dataset-new/Data_Benign_plus_ET/Train_gt/'","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.248151Z","iopub.execute_input":"2022-06-06T08:55:55.248538Z","iopub.status.idle":"2022-06-06T08:55:55.256292Z","shell.execute_reply.started":"2022-06-06T08:55:55.2485Z","shell.execute_reply":"2022-06-06T08:55:55.255578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = 4 \n\ndef create_df(IMAGE_PATH):\n    name = []\n    for dirname, _, filenames in os.walk(IMAGE_PATH):\n        for filename in filenames:\n            name.append(filename.split('.')[0])\n    \n    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n\ndf = create_df(Train_IMAGE_PATH)\nprint('Total Patches: ', len(df))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.258434Z","iopub.execute_input":"2022-06-06T08:55:55.258942Z","iopub.status.idle":"2022-06-06T08:55:55.822095Z","shell.execute_reply.started":"2022-06-06T08:55:55.258904Z","shell.execute_reply":"2022-06-06T08:55:55.82133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Val_IMAGE_PATH = \"../input/dataset-new/Data_Benign_plus_ET/Validation/\"\nVal_MASK_PATH = \"../input/dataset-new/Data_Benign_plus_ET/Validation_gt/\"","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.824775Z","iopub.execute_input":"2022-06-06T08:55:55.824963Z","iopub.status.idle":"2022-06-06T08:55:55.831106Z","shell.execute_reply.started":"2022-06-06T08:55:55.824939Z","shell.execute_reply":"2022-06-06T08:55:55.83047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = create_df(Val_IMAGE_PATH)\nprint('Total Val Images: ', len(df_val))\nprint(df_val)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.832778Z","iopub.execute_input":"2022-06-06T08:55:55.833443Z","iopub.status.idle":"2022-06-06T08:55:55.850657Z","shell.execute_reply.started":"2022-06-06T08:55:55.833407Z","shell.execute_reply":"2022-06-06T08:55:55.84994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['id'][50])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.851912Z","iopub.execute_input":"2022-06-06T08:55:55.852337Z","iopub.status.idle":"2022-06-06T08:55:55.861472Z","shell.execute_reply.started":"2022-06-06T08:55:55.852302Z","shell.execute_reply":"2022-06-06T08:55:55.860433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.863314Z","iopub.execute_input":"2022-06-06T08:55:55.863827Z","iopub.status.idle":"2022-06-06T08:55:55.870816Z","shell.execute_reply.started":"2022-06-06T08:55:55.863792Z","shell.execute_reply":"2022-06-06T08:55:55.8699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(Train_IMAGE_PATH  + df['id'][80] + '.png')\n\nmask = Image.open(Train_MASK_PATH  + df['id'][80] + '_gt.png')\nprint('Image Size', np.asarray(img).shape)\nprint('Mask Size', np.asarray(mask).shape)\n\n\nplt.imshow(img)\nplt.imshow(mask, alpha=0.6)\nplt.title('Picture with Mask Appplied')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:55.871808Z","iopub.execute_input":"2022-06-06T08:55:55.872456Z","iopub.status.idle":"2022-06-06T08:55:56.19294Z","shell.execute_reply.started":"2022-06-06T08:55:55.872383Z","shell.execute_reply":"2022-06-06T08:55:56.19226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_map = [[ 0. , 0. , 0.], [0. , 255. , 0.], [255. , 0., 0.] ,[0. , 0. , 255.] ] #Black[0] , Green[1] , Red[2] , Blue[3]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:56.196115Z","iopub.execute_input":"2022-06-06T08:55:56.196575Z","iopub.status.idle":"2022-06-06T08:55:56.201295Z","shell.execute_reply.started":"2022-06-06T08:55:56.196536Z","shell.execute_reply":"2022-06-06T08:55:56.20063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''This method will convert mask labels(to be trained) from RGB to a 2D image whic holds class labels of the pixels.'''\ndef form_2D_label(mask,class_map):\n    # plt.imshow(mask)\n    \n    mask = mask.astype(\"uint8\")\n    label = np.zeros(mask.shape[:2],dtype= np.uint8)\n    \n    for i, rgb in enumerate(class_map):\n        label[(mask == rgb).all(axis=2)] = i\n    \n    return label","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:56.203431Z","iopub.execute_input":"2022-06-06T08:55:56.20405Z","iopub.status.idle":"2022-06-06T08:55:56.210718Z","shell.execute_reply.started":"2022-06-06T08:55:56.204013Z","shell.execute_reply":"2022-06-06T08:55:56.21004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:56.211972Z","iopub.execute_input":"2022-06-06T08:55:56.212294Z","iopub.status.idle":"2022-06-06T08:55:56.220892Z","shell.execute_reply.started":"2022-06-06T08:55:56.212261Z","shell.execute_reply":"2022-06-06T08:55:56.220009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_count = np.asarray([560201119,  95301323,  29131557, 29131557]) #change these pixel counts for four classes\ntot = np.sum(class_count)\n\nclass_weights = (tot - class_count)/tot\nprint(class_weights)\nweights = torch.tensor(list(class_weights)).to(device, dtype = torch.float)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:56.222228Z","iopub.execute_input":"2022-06-06T08:55:56.22254Z","iopub.status.idle":"2022-06-06T08:55:59.151023Z","shell.execute_reply.started":"2022-06-06T08:55:56.222506Z","shell.execute_reply":"2022-06-06T08:55:59.150275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = albu.Compose([\n    albu.HorizontalFlip(p=0.5),\n    albu.VerticalFlip(p=0.5),\n    albu.CropAndPad (percent = -0.2, keep_size=True, interpolation=cv2.INTER_NEAREST, p=0.5),\n    albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit= 15, interpolation= cv2.INTER_NEAREST,\n                                          border_mode= cv2.BORDER_REPLICATE, p=0.5)\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.152277Z","iopub.execute_input":"2022-06-06T08:55:59.1526Z","iopub.status.idle":"2022-06-06T08:55:59.158717Z","shell.execute_reply.started":"2022-06-06T08:55:59.152563Z","shell.execute_reply":"2022-06-06T08:55:59.157837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch._C import NoneType\nclass CytoDataset(Dataset):\n    \n    def __init__(self, img_path, mask_path, X , transform=transform):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.transform == None:\n            img = cv2.imread(self.img_path  + self.X[idx] + '.png')\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            mask = cv2.imread(self.mask_path  + self.X[idx] + '_gt.png')\n            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        else:\n            img = cv2.imread(self.img_path  + self.X[idx] + '.png')\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            mask = cv2.imread(self.mask_path  + self.X[idx] + '_gt.png')\n            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n            augmentations = transform(image=img,mask=mask)\n            img = augmentations['image']\n            mask = augmentations['mask']\n        # plt.imshow(mask)\n        mask_2 = form_2D_label(mask, class_map)\n        \n        img = (img - img.min())/(img.max() - img.min())\n        #, T.Normalize(self.mean, self.std)\n        t = T.Compose([T.ToTensor()])\n        img = t(img)\n        mask_2 = torch.from_numpy(mask_2).long()\n        # print(img.shape)\n        # print(mask_2.shape)\n            \n        return img, mask_2","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.160061Z","iopub.execute_input":"2022-06-06T08:55:59.160538Z","iopub.status.idle":"2022-06-06T08:55:59.172137Z","shell.execute_reply.started":"2022-06-06T08:55:59.160501Z","shell.execute_reply":"2022-06-06T08:55:59.171468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CytoDataset_val(Dataset):\n    \n    def __init__(self, img_path, mask_path, X , transform=None):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_path  + self.X[idx] + '.png')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.mask_path  + self.X[idx] + '_gt.png')\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        # plt.imshow(mask)\n        mask_2 = form_2D_label(mask, class_map)\n        \n        img = (img - img.min())/(img.max() - img.min())\n        #, T.Normalize(self.mean, self.std)\n        t = T.Compose([T.ToTensor()])\n        img = t(img)\n        mask_2 = torch.from_numpy(mask_2).long()\n        # print(img.shape)\n        # print(mask_2.shape)\n            \n        return img, mask_2","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.173277Z","iopub.execute_input":"2022-06-06T08:55:59.173641Z","iopub.status.idle":"2022-06-06T08:55:59.18419Z","shell.execute_reply.started":"2022-06-06T08:55:59.173605Z","shell.execute_reply":"2022-06-06T08:55:59.183411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = CytoDataset(Train_IMAGE_PATH, Train_MASK_PATH, df['id'].values)\nval_set = CytoDataset_val(Val_IMAGE_PATH, Val_MASK_PATH, df_val['id'].values)\n\n#dataloader\nbatch_size= 1\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)  ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.18552Z","iopub.execute_input":"2022-06-06T08:55:59.186009Z","iopub.status.idle":"2022-06-06T08:55:59.195339Z","shell.execute_reply.started":"2022-06-06T08:55:59.185973Z","shell.execute_reply":"2022-06-06T08:55:59.194646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" print( train_set.__len__())","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.197508Z","iopub.execute_input":"2022-06-06T08:55:59.19814Z","iopub.status.idle":"2022-06-06T08:55:59.204975Z","shell.execute_reply.started":"2022-06-06T08:55:59.19799Z","shell.execute_reply":"2022-06-06T08:55:59.204295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(val_set.__len__())","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.206146Z","iopub.execute_input":"2022-06-06T08:55:59.206876Z","iopub.status.idle":"2022-06-06T08:55:59.211573Z","shell.execute_reply.started":"2022-06-06T08:55:59.206839Z","shell.execute_reply":"2022-06-06T08:55:59.210759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------------------- Res_Unet++ model --------------------------------------\nimport torch.nn as nn\nimport torch\n\n\nclass ResidualConv(nn.Module):\n    def __init__(self, input_dim, output_dim, stride, padding):\n        super(ResidualConv, self).__init__()\n\n        self.conv_block = nn.Sequential(\n            nn.BatchNorm2d(input_dim),\n            nn.ReLU(),\n            nn.Conv2d(\n                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding\n            ),\n            nn.BatchNorm2d(output_dim),\n            nn.ReLU(),\n            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n        )\n        self.conv_skip = nn.Sequential(\n            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(output_dim),\n        )\n\n    def forward(self, x):\n\n        return self.conv_block(x) + self.conv_skip(x)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel, stride):\n        super(Upsample, self).__init__()\n\n        self.upsample = nn.ConvTranspose2d(\n            input_dim, output_dim, kernel_size=kernel, stride=stride\n        )\n\n    def forward(self, x):\n        return self.upsample(x)\n\n\nclass Squeeze_Excite_Block(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(Squeeze_Excite_Block, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n\nclass ASPP(nn.Module):\n    def __init__(self, in_dims, out_dims, rate=[6, 12, 18]):\n        super(ASPP, self).__init__()\n\n        self.aspp_block1 = nn.Sequential(\n            nn.Conv2d(\n                in_dims, out_dims, 3, stride=1, padding=rate[0], dilation=rate[0]\n            ),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_dims),\n        )\n        self.aspp_block2 = nn.Sequential(\n            nn.Conv2d(\n                in_dims, out_dims, 3, stride=1, padding=rate[1], dilation=rate[1]\n            ),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_dims),\n        )\n        self.aspp_block3 = nn.Sequential(\n            nn.Conv2d(\n                in_dims, out_dims, 3, stride=1, padding=rate[2], dilation=rate[2]\n            ),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(out_dims),\n        )\n\n        self.output = nn.Conv2d(len(rate) * out_dims, out_dims, 1)\n        self._init_weights()\n\n    def forward(self, x):\n        x1 = self.aspp_block1(x)\n        x2 = self.aspp_block2(x)\n        x3 = self.aspp_block3(x)\n        out = torch.cat([x1, x2, x3], dim=1)\n        return self.output(out)\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass Upsample_(nn.Module):\n    def __init__(self, scale=2):\n        super(Upsample_, self).__init__()\n\n        self.upsample = nn.Upsample(mode=\"bilinear\", scale_factor=scale)\n\n    def forward(self, x):\n        return self.upsample(x)\n\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, input_encoder, input_decoder, output_dim):\n        super(AttentionBlock, self).__init__()\n\n        self.conv_encoder = nn.Sequential(\n            nn.BatchNorm2d(input_encoder),\n            nn.ReLU(),\n            nn.Conv2d(input_encoder, output_dim, 3, padding=1),\n            nn.MaxPool2d(2, 2),\n        )\n\n        self.conv_decoder = nn.Sequential(\n            nn.BatchNorm2d(input_decoder),\n            nn.ReLU(),\n            nn.Conv2d(input_decoder, output_dim, 3, padding=1),\n        )\n\n        self.conv_attn = nn.Sequential(\n            nn.BatchNorm2d(output_dim),\n            nn.ReLU(),\n            nn.Conv2d(output_dim, 1, 1),\n        )\n\n    def forward(self, x1, x2):\n        out = self.conv_encoder(x1) + self.conv_decoder(x2)\n        out = self.conv_attn(out)\n        return out * x2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T08:55:59.213098Z","iopub.execute_input":"2022-06-06T08:55:59.213672Z","iopub.status.idle":"2022-06-06T08:55:59.240387Z","shell.execute_reply.started":"2022-06-06T08:55:59.213636Z","shell.execute_reply":"2022-06-06T08:55:59.239699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n# from core.modules import (\n#     ResidualConv,\n#     ASPP,\n#     AttentionBlock,\n#     Upsample_,\n#     Squeeze_Excite_Block,\n# )\n\n\nclass ResUnetPlusPlus(nn.Module):\n    def __init__(self, channel, filters=[32, 64, 128, 256, 512]):\n        super(ResUnetPlusPlus, self).__init__()\n\n        self.input_layer = nn.Sequential(\n            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1),\n            nn.BatchNorm2d(filters[0]),\n            nn.ReLU(),\n            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n        )\n        self.input_skip = nn.Sequential(\n            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1)\n        )\n\n        self.squeeze_excite1 = Squeeze_Excite_Block(filters[0])\n\n        self.residual_conv1 = ResidualConv(filters[0], filters[1], 2, 1)\n\n        self.squeeze_excite2 = Squeeze_Excite_Block(filters[1])\n\n        self.residual_conv2 = ResidualConv(filters[1], filters[2], 2, 1)\n\n        self.squeeze_excite3 = Squeeze_Excite_Block(filters[2])\n\n        self.residual_conv3 = ResidualConv(filters[2], filters[3], 2, 1)\n\n        self.aspp_bridge = ASPP(filters[3], filters[4])\n\n        self.attn1 = AttentionBlock(filters[2], filters[4], filters[4])\n        self.upsample1 = Upsample_(2)\n        self.up_residual_conv1 = ResidualConv(filters[4] + filters[2], filters[3], 1, 1)\n\n        self.attn2 = AttentionBlock(filters[1], filters[3], filters[3])\n        self.upsample2 = Upsample_(2)\n        self.up_residual_conv2 = ResidualConv(filters[3] + filters[1], filters[2], 1, 1)\n\n        self.attn3 = AttentionBlock(filters[0], filters[2], filters[2])\n        self.upsample3 = Upsample_(2)\n        self.up_residual_conv3 = ResidualConv(filters[2] + filters[0], filters[1], 1, 1)\n\n        self.aspp_out = ASPP(filters[1], filters[0])\n\n#         self.output_layer = nn.Sequential(nn.Conv2d(filters[0], 1, 1), nn.Sigmoid())\n        self.output_layer = nn.Conv2d(filters[0], 4, 1)\n    def forward(self, x):\n        x1 = self.input_layer(x) + self.input_skip(x)\n\n        x2 = self.squeeze_excite1(x1)\n        x2 = self.residual_conv1(x2)\n\n        x3 = self.squeeze_excite2(x2)\n        x3 = self.residual_conv2(x3)\n\n        x4 = self.squeeze_excite3(x3)\n        x4 = self.residual_conv3(x4)\n\n        x5 = self.aspp_bridge(x4)\n\n        x6 = self.attn1(x3, x5)\n        x6 = self.upsample1(x6)\n        x6 = torch.cat([x6, x3], dim=1)\n        x6 = self.up_residual_conv1(x6)\n\n        x7 = self.attn2(x2, x6)\n        x7 = self.upsample2(x7)\n        x7 = torch.cat([x7, x2], dim=1)\n        x7 = self.up_residual_conv2(x7)\n\n        x8 = self.attn3(x1, x7)\n        x8 = self.upsample3(x8)\n        x8 = torch.cat([x8, x1], dim=1)\n        x8 = self.up_residual_conv3(x8)\n\n        x9 = self.aspp_out(x8)\n        out = self.output_layer(x9)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.241711Z","iopub.execute_input":"2022-06-06T08:55:59.242136Z","iopub.status.idle":"2022-06-06T08:55:59.262634Z","shell.execute_reply.started":"2022-06-06T08:55:59.242098Z","shell.execute_reply":"2022-06-06T08:55:59.261717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom packaging import version\n\nclass CrossEntropy2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight= None):\n        \"\"\"\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size \"nclasses\"\n        \"\"\"\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), \"{0} vs {1} \".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n        loss = F.cross_entropy(predict, target, weight=weight, size_average=self.size_average)\n        return loss\n\n\nclass BCEWithLogitsLoss2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(BCEWithLogitsLoss2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        \"\"\"\n            Args:\n                predict:(n, 1, h, w)\n                target:(n, 1, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size \"nclasses\"\n        \"\"\"\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 4\n        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(2), \"{0} vs {1} \".format(predict.size(2), target.size(2))\n        assert predict.size(3) == target.size(3), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict[target_mask]\n        loss = F.binary_cross_entropy_with_logits(predict, target, weight=weight, size_average=self.size_average)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.266061Z","iopub.execute_input":"2022-06-06T08:55:59.266272Z","iopub.status.idle":"2022-06-06T08:55:59.285661Z","shell.execute_reply.started":"2022-06-06T08:55:59.266246Z","shell.execute_reply":"2022-06-06T08:55:59.28488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = {'IMG_MEAN' : np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32),  #calculate the mean of RGB in real images of train data\n        'MODEL' : 'ResUnetPlusPlus' , 'BATCH_SIZE' : 1,\n'ITER_SIZE' : 1,  'NUM_WORKERS' : 4, \n'IGNORE_LABEL' : 255 , 'INPUT_SIZE' : '512,512' , 'LEARNING_RATE' : 2.5e-4 , 'MOMENTUM' : 0.9,  'NUM_CLASSES' : 4, \n'NUM_STEPS' : 20000, \n'POWER' : 0.9,\n'RANDOM_SEED' : 1234,\n'SAVE_NUM_IMAGES' : 2,\n'SAVE_PRED_EVERY' : 5000,\n'SNAPSHOT_DIR' : './snapshots/',\n'WEIGHT_DECAY' : 0.0005,\n\n'LEARNING_RATE_D' : 1e-4,\n'LAMBDA_ADV_PRED' : 0.1,\n'PARTIAL_DATA' : None, #0.5,\n\n'SEMI_START' : 5000,\n'LAMBDA_SEMI' : 0.1,\n'MASK_T' : 0.2,\n\n'LAMBDA_SEMI_ADV':0.001,\n'SEMI_START_ADV' : 0,\n'D_REMAIN' : False, 'GPU':True,\n       'RESTORE_FROM' : 'http://vllab1.ucmerced.edu/~whung/adv-semi-seg/resnet101COCO-41f33a49.pth'}","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.288545Z","iopub.execute_input":"2022-06-06T08:55:59.290067Z","iopub.status.idle":"2022-06-06T08:55:59.297271Z","shell.execute_reply.started":"2022-06-06T08:55:59.290027Z","shell.execute_reply":"2022-06-06T08:55:59.296576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import copy_reg\nexcept:\n    import copyreg as copy_reg","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.298865Z","iopub.execute_input":"2022-06-06T08:55:59.3001Z","iopub.status.idle":"2022-06-06T08:55:59.30952Z","shell.execute_reply.started":"2022-06-06T08:55:59.300063Z","shell.execute_reply":"2022-06-06T08:55:59.308843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pixel_accuracy(output, mask):\n    with torch.no_grad():\n        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n        correct = torch.eq(output, mask).int()\n        accuracy = float(correct.sum()) / float(correct.numel())\n    return accuracy\n\ndef mIoU(pred_mask, mask, smooth=1e-10, n_classes=5):\n    with torch.no_grad():\n        pred_mask = F.softmax(pred_mask, dim=1)\n        pred_mask = torch.argmax(pred_mask, dim=1)\n        pred_mask = pred_mask.contiguous().view(-1)\n        mask = mask.contiguous().view(-1)\n\n        iou_per_class = []\n        for clas in range(0, n_classes): #loop per pixel class\n            true_class = pred_mask == clas\n            true_label = mask == clas\n\n            if true_label.long().sum().item() == 0: #no exist label in this loop\n                iou_per_class.append(np.nan)\n            else:\n                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n                union = torch.logical_or(true_class, true_label).sum().float().item()\n\n                iou = (intersect + smooth) / (union +smooth)\n                iou_per_class.append(iou)\n        return np.nanmean(iou_per_class)\n    \ndef map_thiss(y_pred,class_map):\n    y_pred_rgb = np.zeros((y_pred.shape[0],y_pred.shape[1],y_pred.shape[2],3))\n    for i in range(y_pred.shape[0]):\n        image = np.zeros((y_pred.shape[1],y_pred.shape[2],3))\n        for j in range(y_pred.shape[1]):\n            for k in range(y_pred.shape[2]):\n                image[j,k,:] = class_map[int(y_pred[i][j][k])]\n        y_pred_rgb[i] = image\n    return y_pred_rgb\n\ndef plot_result(img, title):\n    plt.figure(figsize=(12, 6))\n    plt.title(title)\n    plt.imshow(img[0])\n    plt.show()\n    \ndef export_model(model, optimizer=None, name=None, step=None):\n\n        # set output filename\n        if name is not None:\n            out_file = name\n        else:\n            out_file = \"checkpoint\"\n        if step is not None:\n            out_file += \"_step_\" + str(step)\n            \n        out_file = os.path.join(\"./\", out_file + \".pth\")\n\n        # save model\n        data = {\"model_state_dict\": model.state_dict()}\n        if step is not None:\n            data[\"step\"] = step\n        if optimizer is not None:\n            data[\"optimizer_state_dict\"] = optimizer.state_dict()\n        torch.save(data, out_file)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.310974Z","iopub.execute_input":"2022-06-06T08:55:59.311233Z","iopub.status.idle":"2022-06-06T08:55:59.329278Z","shell.execute_reply.started":"2022-06-06T08:55:59.311196Z","shell.execute_reply":"2022-06-06T08:55:59.32848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef loss_calc(pred, label, gpu):\n    \"\"\"\n    This function returns cross entropy loss for semantic segmentation\n    \"\"\"\n    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n    label = Variable(label.long()).cuda(device)\n    criterion = CrossEntropy2d().cuda(device)\n\n    return criterion(pred, label)\n\n\ndef lr_poly(base_lr, iter, max_iter, power):\n    return base_lr*((1-float(iter)/max_iter)**(power))\n\n\ndef adjust_learning_rate(optimizer, i_iter):\n    lr = lr_poly(args[\"LEARNING_RATE\"], i_iter, args[\"NUM_STEPS\"], args[\"POWER\"])\n    optimizer.param_groups[0]['lr'] = lr\n    if len(optimizer.param_groups) > 1 :\n        optimizer.param_groups[1]['lr'] = lr * 10\n\ndef adjust_learning_rate_D(optimizer, i_iter):\n    lr = lr_poly(args[\"LEARNING_RATE_D\"], i_iter, args[\"NUM_STEPS\"], args[\"POWER\"])\n    optimizer.param_groups[0]['lr'] = lr\n    if len(optimizer.param_groups) > 1 :\n        optimizer.param_groups[1]['lr'] = lr * 10\n\ndef one_hot(label):\n    label = label.numpy()\n    one_hot = np.zeros((label.shape[0], args[\"NUM_CLASSES\"], label.shape[1], label.shape[2]), dtype=label.dtype)\n    for i in range(args[\"NUM_CLASSES\"]):\n        one_hot[:,i,...] = (label==i)\n    #handle ignore labels\n    return torch.FloatTensor(one_hot)\n\ndef make_D_label(label, ignore_mask):\n    ignore_mask = np.expand_dims(ignore_mask, axis=1)\n    D_label = np.ones(ignore_mask.shape)*label\n    D_label[ignore_mask] = 255\n    D_label = Variable(torch.FloatTensor(D_label)).cuda(device)\n\n    return D_label","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.331907Z","iopub.execute_input":"2022-06-06T08:55:59.333713Z","iopub.status.idle":"2022-06-06T08:55:59.345477Z","shell.execute_reply.started":"2022-06-06T08:55:59.333592Z","shell.execute_reply":"2022-06-06T08:55:59.344736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n    torch.cuda.empty_cache()\n    train_losses = []\n    test_losses = []\n    val_iou = []; val_acc = []\n    train_iou = []; train_acc = []\n    lrs = []\n    min_loss = np.inf\n    decrease = 1 ; not_improve=0\n\n    model.to(device)\n    fit_time = time.time()\n    for e in range(epochs):\n        since = time.time()\n        running_loss = 0\n        iou_score = 0\n        accuracy = 0\n        #training loop\n        model.train()\n        for i, data in enumerate(tqdm(train_loader)):\n            #training phase\n            image_tiles, mask_tiles = data\n            # print(image_tiles.shape)\n            # print(mask_tiles.shape)\n            image = image_tiles.to(device, dtype = torch.float); mask = mask_tiles.to(device, dtype= torch.float);\n            #forward\n            output = model(image)\n            loss = loss_calc(output, mask, device)\n            #evaluation metrics\n            iou_score += mIoU(output, mask)\n            accuracy += pixel_accuracy(output, mask)\n            #backward\n            loss.backward()\n            optimizer.step() #update weight          \n            optimizer.zero_grad() #reset gradient\n            \n            #step the learning rate\n            lrs.append(get_lr(optimizer))\n            scheduler.step() \n            \n            running_loss += loss.item()\n            \n        model.eval()\n        test_loss = 0\n        test_accuracy = 0\n        val_iou_score = 0\n        #validation loop\n        with torch.no_grad():\n            for i, data in enumerate(tqdm(val_loader)):\n                #reshape to 9 patches from single image, delete batch size\n                image_tiles, mask_tiles = data\n\n\n                image = image_tiles.to(device, dtype = torch.float); mask = mask_tiles.to(device, dtype = torch.float);\n                output = model(image)\n                #evaluation metrics\n                if(i==1):\n                    output_soft = F.softmax(output, dim=1)\n                    output_num = output_soft.cpu().detach().numpy()\n                    pred_mask = np.argmax(output_num, axis = 1)\n        \n                    y_pred_rgb = map_thiss(pred_mask,class_map)\n                    y_test_rgb = map_thiss(mask,class_map)\n                    plot_result(y_test_rgb,\"Original Masks\")\n                    plot_result(y_pred_rgb,\"Predicted Masks\")\n                val_iou_score +=  mIoU(output, mask)\n                test_accuracy += pixel_accuracy(output, mask)\n                #loss\n                loss = loss_calc(output, mask, device)                                  \n                test_loss += loss.item()\n            \n        #calculatio mean for each batch\n        train_losses.append(running_loss/len(train_loader))\n        test_losses.append(test_loss/len(val_loader))\n\n\n        if min_loss > (test_loss/len(val_loader)):\n            print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n            min_loss = (test_loss/len(val_loader))\n            decrease += 1\n            #if decrease % 5 == 0:\n            print('saving model...')\n            export_model(model, optimizer=optimizer, name=\"final\", step = e)\n            #torch.save(model, 'Unet-Mobilenet_v2_val_loss-{:.3f}.pt'.format(test_loss/len(val_loader)))\n\n\n        if (test_loss/len(val_loader)) > min_loss:\n            not_improve += 1\n            min_loss = (test_loss/len(val_loader))\n            print(f'Loss Not Decrease for {not_improve} time')\n            #if not_improve == 7:\n                #print('Loss not decrease for 7 times, Stop Training')\n                #break\n\n        #iou\n        val_iou.append(val_iou_score/len(val_loader))\n        train_iou.append(iou_score/len(train_loader))\n        train_acc.append(accuracy/len(train_loader))\n        val_acc.append(test_accuracy/ len(val_loader))\n        print(\"Epoch:{}/{}..\".format(e+1, epochs),\n              \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n              \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n              \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n              \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n              \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n              \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n              \"Time: {:.2f}m\".format((time.time()-since)/60))\n        \n    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n               'train_miou' :train_iou, 'val_miou':val_iou,\n               'train_acc' :train_acc, 'val_acc':val_acc,\n               'lrs': lrs}\n    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.346879Z","iopub.execute_input":"2022-06-06T08:55:59.347349Z","iopub.status.idle":"2022-06-06T08:55:59.371028Z","shell.execute_reply.started":"2022-06-06T08:55:59.347312Z","shell.execute_reply":"2022-06-06T08:55:59.370354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CUDA_LAUNCH_BLOCKING=1","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.37565Z","iopub.execute_input":"2022-06-06T08:55:59.375888Z","iopub.status.idle":"2022-06-06T08:55:59.381107Z","shell.execute_reply.started":"2022-06-06T08:55:59.375857Z","shell.execute_reply":"2022-06-06T08:55:59.380417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_lr = 1e-3\nepoch = 50\nweight_decay = 1e-4\n\nmodel = ResUnetPlusPlus(3)\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\nsched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n                                            steps_per_epoch=len(train_loader))\n\n\n\nhistory = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)\n# history = fit(epoch, model, val_loader, val_loader, criterion, optimizer, sched)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:55:59.382499Z","iopub.execute_input":"2022-06-06T08:55:59.383066Z","iopub.status.idle":"2022-06-06T08:56:18.213955Z","shell.execute_reply.started":"2022-06-06T08:55:59.383031Z","shell.execute_reply":"2022-06-06T08:56:18.212606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_lr = 1e-3\n# epoch = 5\n# weight_decay = 1e-4\n\n# model = ResUnetPlusPlus(3)\n# criterion = nn.CrossEntropyLoss()\n\n# state = torch.load('../input/weights-file/final_step_45.pth', map_location = 'cpu')\n# model.load_state_dict(state['model_state_dict'])\n\n# model.cuda()\n\n\n# optimizer = torch.optim.AdamW(model.parameters())\n\n# optimizer.load_state_dict(state['optimizer_state_dict'])\n# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n#                                             steps_per_epoch=len(train_loader))\n\n\n\n# history = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:56:18.215075Z","iopub.status.idle":"2022-06-06T08:56:18.215818Z","shell.execute_reply.started":"2022-06-06T08:56:18.215558Z","shell.execute_reply":"2022-06-06T08:56:18.215584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(history):\n    plt.plot(history['val_loss'], label='val', marker='o')\n    plt.plot( history['train_loss'], label='train', marker='o')\n    plt.title('Loss per epoch'); plt.ylabel('loss');\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()\n    \ndef plot_score(history):\n    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()\n    \ndef plot_acc(history):\n    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:56:18.21708Z","iopub.status.idle":"2022-06-06T08:56:18.217845Z","shell.execute_reply.started":"2022-06-06T08:56:18.21759Z","shell.execute_reply":"2022-06-06T08:56:18.217615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss(history)\nplot_score(history)\nplot_acc(history)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:56:18.218984Z","iopub.status.idle":"2022-06-06T08:56:18.219743Z","shell.execute_reply.started":"2022-06-06T08:56:18.219487Z","shell.execute_reply":"2022-06-06T08:56:18.219513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./history.pickle', 'wb') as handle:\n    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:56:18.220884Z","iopub.status.idle":"2022-06-06T08:56:18.221635Z","shell.execute_reply.started":"2022-06-06T08:56:18.221394Z","shell.execute_reply":"2022-06-06T08:56:18.22142Z"},"trusted":true},"execution_count":null,"outputs":[]}]}